{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Weng Yuqian\n",
    "\n",
    "**EID:** 55972965\n",
    "\n",
    "**Kaggle Team Name:** Cccc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 1 - Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_data(fname):\n",
    "    txtdata = []\n",
    "    classes = []\n",
    "    topics  = []\n",
    "    with open(fname, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # get the text\n",
    "            txtdata.append(row[0])\n",
    "            # get the class (convert to integer)\n",
    "            if len(row)>1:\n",
    "                classes.append(row[1])\n",
    "                topics.append(row[2])\n",
    "    \n",
    "    if (len(classes)>0) and (len(txtdata) != len(classes)):        \n",
    "        raise Exception(\"mismatched length!\")\n",
    "    \n",
    "    return (txtdata, classes, topics)\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "    \n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "    \n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "        \n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2396\n",
      "1028\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "# (if using Kaggle notebooks you need to include the directory path: /kaggle/input/cs5489-2020b-assignment-1/)\n",
    "(traintxt, trainY, traintopic) = read_text_data(\"sanders_tweets_train.txt\")\n",
    "(testtxt, _, _)                = read_text_data(\"sanders_tweets_test.txt\")\n",
    "\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "- At first, I use tf-idf to du feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = feature_extraction.text.TfidfVectorizer (norm='l1', stop_words='english', max_features=850)\n",
    "features = tfidf_vectorizer.fit_transform(traintxt)\n",
    "testtf=tfidf_vectorizer.fit_transform(testtxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinormial NB\n",
    "- I split the whole training set into two parts of 8:2, set 8 of them as new training set.\n",
    "- Then I use the new training set to seleclt best alpha of multinomial naive bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha= 0.00023101297000831605 , best accuracy= 0.675\n"
     ]
    }
   ],
   "source": [
    "alphas = logspace(-6,3,100)\n",
    "accuracies = []\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(features, trainY, test_size = 0.2, random_state = 0)\n",
    "for al in alphas:\n",
    "    mmodel_tf = naive_bayes.MultinomialNB(alpha=al)\n",
    "    mmodel_tf.fit(X_train, y_train)\n",
    "    tf_predictY = mmodel_tf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, tf_predictY)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "n_bestalpha = argmax(accuracies)\n",
    "bestalpha = alphas[n_bestalpha]\n",
    "bestaccuracy = accuracies[n_bestalpha]\n",
    "print(\"best alpha=\", bestalpha, \", best accuracy=\", bestaccuracy)\n",
    "mnb_clf = naive_bayes.MultinomialNB(alpha=bestalpha).fit(X_train, y_train)\n",
    "predictY_mnb = mnb_clf.predict(testtf)\n",
    "# write_csv_kaggle_sub(\"my_submission1_MNB.csv\", predictY_mnb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see from the result, the best accuracy is only 0.675, it seems multinormial naive bayes is not performanced well in this dataset.\n",
    "- The selected best alpha is too small, which means the data is not smooth.\n",
    "- The final predict result of the test set is also not performanced well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "- Then I tried use pipeline to do the svm classifier with count vectorizer and tf-idf transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.7416666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "clf = Pipeline([('vect', feature_extraction.text.CountVectorizer()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', SVC(C=0.99, kernel = 'linear'))])\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(traintxt, trainY, test_size = 0.2, random_state = 0)\n",
    "svm_clf=clf.fit(X_train,y_train)\n",
    "pred=clf.predict(X_test)\n",
    "acc = metrics.accuracy_score(pred, y_test)\n",
    "print(\"test accuracy:\",acc)\n",
    "svm_clf = clf.fit(traintxt, trainY)\n",
    "predictY_svm = svm_clf.predict(testtxt)\n",
    "write_csv_kaggle_sub(\"my_submission.csv\", predictY_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the test accuracy we can see, it performanced better than multinormial naive bayes classifier.\n",
    "- In fact, this result is the best result I get, and also my best submission on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   27.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1200 out of 1200 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__C: 2.154434690031882\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__max_df: 0.6\n",
      "\tvect__max_features: None\n",
      "test accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__max_df': ( 0.6, 0.7, 0.8, 0.9),'vect__max_features': (None, 500, 800),\n",
    "              'tfidf__use_idf': (True, False),'clf__C': (logspace(-3,3,10))}\n",
    "gs = model_selection.GridSearchCV(svm_clf, parameters, n_jobs=-1, verbose=1,cv=5)\n",
    "gs.fit(X_train,y_train)\n",
    "best_parameters = dict()\n",
    "best_parameters = gs.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "pred=gs.predict(X_test)\n",
    "acc = metrics.accuracy_score(pred, y_test)\n",
    "print(\"test accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then I try to optimize the parameters of the svm classifier by using cross validation.\n",
    "- It shows the test accuracy is greater than before, and also select some best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictY_svm2 = gs.predict(testtxt)\n",
    "# write_csv_kaggle_sub(\"my_submission3_svm.csv\", predictY_svm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But the final result is not as good as before, and I cannot figure out why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "- I use xgboost, i.e. gradient boosting decision tree, with cross validation.\n",
    "- It can use all CPU cores to build trees in parallel, and the prediction is precise in usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 18.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'colsample_bytree': 0.8717140047046047, 'gamma': 0.42769285628251436, 'learning_rate': 0.05461018994592304, 'max_depth': 2, 'n_estimators': 926, 'subsample': 0.8285177730578381}\n",
      "gradient boosting test accuracy = 0.7145833333333333\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "paramsampler = {    \n",
    "    \"colsample_bytree\": stats.uniform(0.7, 0.3),  \n",
    "    \"gamma\":            stats.uniform(0, 0.5),    \n",
    "    \"max_depth\":        stats.randint(2, 6),      \n",
    "    \"subsample\":        stats.uniform(0.6, 0.4),  \n",
    "    \"learning_rate\":    stats.uniform(.001,1),    \n",
    "    \"n_estimators\":     stats.randint(10, 1000),\n",
    "}\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(features, trainY, test_size = 0.2, random_state = 0)\n",
    "xclf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=4487)\n",
    "xgbrcv = model_selection.RandomizedSearchCV(xclf, param_distributions=paramsampler, \n",
    "                            random_state=4487, n_iter=200, cv=5, \n",
    "                            verbose=1, n_jobs=-1)\n",
    "xgbrcv.fit(X_train, y_train)\n",
    "print(\"best params:\", xgbrcv.best_params_)\n",
    "gbpredY = xgbrcv.predict(X_test)\n",
    "acc = metrics.accuracy_score(y_test, gbpredY)\n",
    "print(\"gradient boosting test accuracy =\", acc)\n",
    "predictY_xgb = xgbrcv.predict(testtf)\n",
    "# write_csv_kaggle_sub(\"my_submission4_xgb.csv\", predictY_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The test accuracy is not well compared to svm classifier.\n",
    "- The final result is much worse than I imagined, even worse than multinormial naive bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textblob\n",
    "- **I found it when I search the relevant information. It seem insteresting.**\n",
    "- TextBlob is an open source text processing library written in Python. It can be used to perform many natural language processing tasks, such as part-of-speech tagging, nominal component extraction, sentiment analysis, text translation, etc.\n",
    "- It is easy to use to do sentiment analysis. If analysis.sentiment.polarity > 0, it means the input data represent positive, if analysis.sentiment.polarity = 0, it means neutral, and if analysis.sentiment.polarity < 0, it means negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'negative', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'neutral', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'neutral', 'negative', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'negative', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'negative', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'negative', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'negative', 'negative', 'positive', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'neutral', 'negative', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'negative', 'neutral', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'negative', 'positive', 'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'negative', 'positive', 'positive', 'negative', 'positive', 'neutral', 'negative', 'negative', 'negative', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'neutral', 'negative', 'neutral', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'positive', 'positive', 'positive', 'neutral', 'positive', 'negative', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive', 'neutral', 'neutral', 'neutral', 'negative', 'negative', 'neutral', 'neutral', 'negative', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive', 'negative', 'positive', 'neutral', 'positive', 'neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'positive', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'neutral', 'neutral', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "Y = []\n",
    "for i in range(0,len(testtxt)):\n",
    "    analysis = TextBlob(testtxt[i])\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        Y.append(\"positive\") \n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        Y.append(\"neutral\")\n",
    "    else:\n",
    "        Y.append(\"negative\")\n",
    "# write_csv_kaggle_sub(\"my_submission6.csv\", Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But the final result is still not good as svm classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SVD to do feature extraction\n",
    "- I also trie use SVD to do the feature extraction, but the result is too bad, it seems cannot predict the positive tweet, why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(features)\n",
    "xtrain_svd = svd.transform(features)\n",
    "xtest_svd = svd.transform(testtf)\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'neutral' ... 'neutral' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([('vect', decomposition.TruncatedSVD()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', SVC(C=0.99, kernel = 'linear'))])\n",
    "testclf = clf.fit(xtrain_svd_scl, trainY)\n",
    "pred = testclf.predict(xtest_svd_scl)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
